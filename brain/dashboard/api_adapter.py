from flask import Blueprint, jsonify, request
from datetime import datetime
import sqlite3
import json
from typing import List, Dict, Any

# Import internal modules from the "Brain"
from db_setup import get_connection

# Wrap optional imports to prevent crash if Scholar/Google libs missing
try:
    from scholar.brain_reader import (
        get_all_sessions,
        get_session_by_id,
        get_session_count,
        calculate_session_metrics,
    )
    from scholar.friction_alerts import generate_alerts
except ImportError:
    # Scholar modules are optional - provide fallbacks silently
    get_all_sessions = lambda limit=None: []
    get_session_by_id = lambda x: None
    get_session_count = lambda: 0
    calculate_session_metrics = lambda: {}
    generate_alerts = lambda: []

from dashboard.syllabus import fetch_all_courses_and_events

# Define the Blueprint that mimics the Node.js API
adapter_bp = Blueprint("api_adapter", __name__, url_prefix="/api")

# ==============================================================================
# SESSIONS
# ==============================================================================


@adapter_bp.route("/sessions", methods=["GET"])
def get_sessions():
    """
    Mimics: app.get("/api/sessions")
    Returns a list of study sessions.
    """
    raw_sessions = get_all_sessions()

    serialized = []
    for s in raw_sessions:
        # SessionRecord is a dataclass, convert to dict
        s_dict = s.__dict__ if hasattr(s, "__dict__") else s

        # safely get values
        understanding = s_dict.get("understanding_level")
        if understanding is None:
            understanding = 0

        # Robust date construction
        raw_date = s_dict.get("session_date")
        if raw_date and s_dict.get("session_time"):
            date_str = f"{raw_date}T{s_dict.get('session_time')}"
        else:
            date_str = raw_date

        final_date = safe_iso_date(date_str) or datetime.now().isoformat()

        mapped = {
            "id": s_dict.get("id"),
            "type": "study",  # hardcoded for now or map from study_mode
            "topic": s_dict.get("main_topic")
            or s_dict.get("topic")
            or "Untitled Session",
            "date": final_date,
            "durationMinutes": s_dict.get("duration_minutes", 0),
            "understanding": understanding,
            "cards": s_dict.get("anki_cards_count", 0) or 0,
            "errors": 0,
        }
        serialized.append(mapped)

    return jsonify(serialized)


@adapter_bp.route("/sessions/stats", methods=["GET"])
def get_session_stats():
    """
    Mimics: app.get("/api/sessions/stats")
    """
    from scholar.brain_reader import get_session_count, get_average_metrics

    count = get_session_count()
    avgs = get_average_metrics()

    return jsonify(
        {
            "total": count,
            "avgErrors": 0,  # Placeholder
            "totalCards": int(
                avgs.get("avg_duration_minutes", 0) * count * 0.5
            ),  # Estimate for now
        }
    )


@adapter_bp.route("/sessions/<int:session_id>", methods=["GET"])
def get_single_session(session_id):
    session = get_session_by_id(session_id)
    if not session:
        return jsonify({"error": "Session not found"}), 404

    s_dict = session.__dict__ if hasattr(session, "__dict__") else session
    return jsonify(s_dict)


@adapter_bp.route("/sessions", methods=["POST"])
def create_session():
    """
    Mimics: app.post("/api/sessions")
    Creates a real session in the SQLite DB.
    """
    data = request.json

    topic = data.get("topic", "Untitled")
    date_str = datetime.now().strftime("%Y-%m-%d")
    time_str = datetime.now().strftime("%H:%M:%S")

    # Manual insertion since brain_reader is read-only
    conn = get_connection()
    cursor = conn.cursor()

    try:
        cursor.execute(
            """
            INSERT INTO sessions (
                session_date, session_time, main_topic, study_mode, 
                created_at, duration_minutes, understanding_level
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                date_str,
                time_str,
                topic,
                "Core",  # Default mode
                datetime.now().isoformat(),
                60,  # Default duration target
                3,  # Default neutral understanding
            ),
        )
        conn.commit()
        new_id = cursor.lastrowid
    except Exception as e:
        conn.rollback()
        return jsonify({"error": str(e)}), 500
    finally:
        conn.close()

    return jsonify(
        {
            "id": new_id,
            "topic": topic,
            "date": f"{date_str}T{time_str}",
            "type": "study",
        }
    ), 201


@adapter_bp.route("/sessions/<int:session_id>", methods=["PATCH"])
def update_session(session_id):
    """Update session details."""
    data = request.json
    try:
        conn = get_connection()
        cur = conn.cursor()

        fields = []
        values = []

        if "topic" in data:
            fields.append("main_topic = ?")
            values.append(data["topic"])
        if "understanding" in data:
            fields.append("understanding_level = ?")
            values.append(data["understanding"])
        if "durationMinutes" in data:
            fields.append("duration_minutes = ?")
            values.append(data["durationMinutes"])

        if not fields:
            return jsonify({"success": True})

        values.append(session_id)
        cur.execute(
            f"UPDATE sessions SET {', '.join(fields)} WHERE id = ?", tuple(values)
        )
        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/sessions/<int:session_id>", methods=["DELETE"])
def delete_session(session_id):
    """Delete a session."""
    try:
        conn = get_connection()
        cur = conn.cursor()
        cur.execute("DELETE FROM sessions WHERE id = ?", (session_id,))
        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ==============================================================================
# EVENTS (Calendar)
# ==============================================================================


# Helper for date safety
def safe_iso_date(date_val):
    if not date_val:
        return None
    try:
        if isinstance(date_val, str):
            if "T" in date_val:
                return date_val
            return f"{date_val}T00:00:00"
        if hasattr(date_val, "isoformat"):
            return date_val.isoformat()
    except:
        pass
    return None


@adapter_bp.route("/events", methods=["GET"])
def get_events():
    """
    Mimics: app.get("/api/events")
    Sources from 'course_events' table via syllabus.py helper.
    """
    from brain.dashboard.syllabus import fetch_all_courses_and_events

    try:
        courses, events = fetch_all_courses_and_events()
        course_map = {c["id"]: c for c in courses}

        serialized = []
        for ev in events:
            # Date safety: Ensure we have a valid date
            raw_start = ev.get("date") or ev.get("due_date")
            start_date = safe_iso_date(raw_start)

            if not start_date:
                continue

            c_info = course_map.get(ev.get("course_id"), {})
            title = ev.get("title", "Untitled")

            serialized.append(
                {
                    "id": ev["id"],
                    "title": title,
                    "date": start_date,
                    "endDate": ev.get("due_date"),
                    "allDay": True,
                    "eventType": (ev.get("type") or "event").lower(),
                    "course": c_info.get("code") or c_info.get("name"),
                    "color": c_info.get("color") or "#ef4444",
                    "status": ev.get("status", "pending"),
                }
            )

        return jsonify(serialized)
    except Exception as e:
        print(f"Calendar Error: {e}")
        return jsonify([]), 500


@adapter_bp.route("/events/<int:event_id>", methods=["DELETE"])
def delete_event(event_id):
    """Delete a local course event."""
    from db_setup import get_connection

    try:
        conn = get_connection()
        cur = conn.cursor()
        cur.execute("DELETE FROM course_events WHERE id = ?", (event_id,))
        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/events/<int:event_id>", methods=["PATCH"])
def update_event(event_id):
    """Update a local course event."""
    data = request.json
    try:
        conn = get_connection()
        cur = conn.cursor()

        fields = []
        values = []

        # Map frontend fields to DB columns
        if "title" in data:
            fields.append("title = ?")
            values.append(data["title"])
        if "date" in data:
            # handle iso
            dt = data["date"].split("T")[0] if "T" in data["date"] else data["date"]
            fields.append("date = ?")
            values.append(dt)
        if "status" in data:
            fields.append("status = ?")
            values.append(data["status"])

        if not fields:
            return jsonify({"success": True})

        values.append(event_id)
        cur.execute(
            f"UPDATE course_events SET {', '.join(fields)} WHERE id = ?", tuple(values)
        )
        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ==============================================================================
# TASKS
# ==============================================================================


@adapter_bp.route("/tasks", methods=["GET"])
def get_tasks():
    """
    Mimics: app.get("/api/tasks")
    Maps incomplete course assignments to Tasks.
    """
    from brain.db_setup import get_connection

    tasks = []
    try:
        conn = get_connection()
        cur = conn.cursor()

        # Fetch Course Assignments that are pending
        cur.execute("""
            SELECT id, title, status, created_at
            FROM course_events
            WHERE type IN ('assignment', 'exam', 'quiz') 
              AND status != 'completed'
            ORDER BY created_at DESC
        """)
        assignment_rows = cur.fetchall()
        for r in assignment_rows:
            # r[3] is createdAt
            created_at = safe_iso_date(r[3]) or datetime.now().isoformat()

            tasks.append(
                {
                    "id": r[0],
                    "title": r[1],
                    "status": "pending",
                    "createdAt": created_at,
                }
            )

        conn.close()
    except Exception as e:
        print(f"Task Fetch Error: {e}")
        return jsonify([]), 500

    return jsonify(tasks)


@adapter_bp.route("/proposals", methods=["GET"])
def get_proposals():
    """
    Mimics: app.get("/api/proposals")
    Maps 'scholar_proposals' to Proposals.
    """
    # get_connection already imported at top of file

    proposals = []

    try:
        conn = get_connection()
        cur = conn.cursor()
        cur.execute("""
            SELECT id, title, proposal_type, status, created_at, filename 
            FROM scholar_proposals 
            WHERE status != 'superseded'
            ORDER BY created_at DESC
        """)
        rows = cur.fetchall()

        for r in rows:
            created_at = safe_iso_date(r[4]) or datetime.now().isoformat()

            proposals.append(
                {
                    "id": r[0],
                    "proposalId": r[5] or str(r[0]),
                    "summary": r[1] or "Untitled Proposal",
                    "status": (r[3] or "DRAFT").upper(),
                    "priority": "MED",
                    "targetSystem": r[2],
                    "createdAt": created_at,
                }
            )
        conn.close()
    except Exception as e:
        print(f"Proposal Fetch Error: {e}")
        return jsonify([]), 500

    return jsonify(proposals)


@adapter_bp.route("/proposals", methods=["POST"])
def create_proposal():
    """Create a new proposal."""
    data = request.json
    try:
        conn = get_connection()
        cur = conn.cursor()

        # proposalId from frontend is used as filename/identifier
        proposal_id = data.get("proposalId")
        if not proposal_id:
            # Auto-generate if missing
            cur.execute("SELECT COUNT(*) FROM scholar_proposals")
            count = cur.fetchone()[0]
            proposal_id = f"P-{count + 101}"

        summary = data.get("summary", "Untitled")
        target_system = data.get("targetSystem", "General")
        status = data.get("status", "DRAFT").lower()
        now = datetime.now().isoformat()

        # We need a filepath as per schema.
        # We'll point to a conceptual path.
        filepath = f"brain/scholar/proposals/{proposal_id}.md"

        cur.execute(
            """
            INSERT INTO scholar_proposals 
            (filename, filepath, title, proposal_type, status, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (proposal_id, filepath, summary, target_system, status, now),
        )

        new_id = cur.lastrowid
        conn.commit()
        conn.close()

        return jsonify(
            {
                "id": new_id,
                "proposalId": proposal_id,
                "summary": summary,
                "status": status.upper(),
                "priority": "MED",
                "targetSystem": target_system,
                "createdAt": now,
            }
        ), 201
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/proposals/<int:prop_id>", methods=["PATCH"])
def update_proposal(prop_id):
    """Update a proposal."""
    data = request.json
    try:
        conn = get_connection()
        cur = conn.cursor()

        fields = []
        values = []

        if "status" in data:
            fields.append("status = ?")
            values.append(data["status"].lower())

        # Frontend might mock other fields in UI but only sends status updates mostly?
        # scholar.tsx updateMutation sends {id, data}.

        if not fields:
            return jsonify({"success": True})

        values.append(prop_id)
        cur.execute(
            f"UPDATE scholar_proposals SET {', '.join(fields)} WHERE id = ?",
            tuple(values),
        )
        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/proposals/<int:prop_id>", methods=["DELETE"])
def delete_proposal(prop_id):
    """Delete a proposal."""
    try:
        conn = get_connection()
        cur = conn.cursor()
        cur.execute("DELETE FROM scholar_proposals WHERE id = ?", (prop_id,))
        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/tasks", methods=["POST"])
def create_task():
    """
    Mimics: app.post("/api/tasks")
    """
    # TODO: Implement manual task creation if needed
    return jsonify({"id": 999, "status": "mocked"}), 201


# ==============================================================================
# SCHOLAR AGENT CONTROL
# ==============================================================================


@adapter_bp.route("/scholar/run", methods=["POST"])
def run_scholar():
    """
    Triggers the Scholar agent loop.
    Uses subprocess to spawn independent process or background thread?
    For safety, let's use a background thread calling `scholar.run_scholar_orchestrator` if properly isolated,
    or subprocess to run `run_scholar.bat` (unattended).

    Subprocess is safer to avoid blocking Flask.
    """
    import subprocess
    from pathlib import Path

    try:
        repo_root = Path(__file__).parent.parent.parent.resolve()
        script_path = repo_root / "scripts" / "run_scholar.bat"

        # Check if running? (Optional)

        # Spawn process
        # We use a special flag or mode? The bat file shows a menu.
        # We need an unattended mode.
        # The user wants "Unattended execution" from menu.
        # But for now, let's just assume we can call the python script directly.

        # Direct python call to bypass menu:
        # python brain/dashboard/scholar.py --mode orchestrator

        py_script = repo_root / "brain" / "dashboard" / "scholar.py"

        # Use Popen to run in background
        subprocess.Popen(
            ["python", str(py_script), "--mode", "orchestrator"],
            cwd=str(repo_root),
            start_new_session=True,  # Detach
        )

        return jsonify({"success": True, "message": "Scholar process started"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/scholar/status", methods=["GET"])
def scholar_status():
    """Check if Scholar is running."""
    from pathlib import Path

    try:
        repo_root = Path(__file__).parent.parent.parent.resolve()
        run_dir = repo_root / "scholar" / "outputs" / "orchestrator_runs"

        is_running = False
        if run_dir.exists():
            for pid_file in run_dir.glob("*.pid"):
                # If pid file exists, check if process is still running
                is_running = True
                break

        return jsonify(
            {"running": is_running, "status": "active" if is_running else "idle"}
        )
    except:
        return jsonify({"running": False, "status": "unknown"})


@adapter_bp.route("/scholar/logs", methods=["GET"])
def scholar_logs():
    """Get latest logs."""
    from pathlib import Path

    try:
        repo_root = Path(__file__).parent.parent.parent.resolve()
        run_dir = repo_root / "scholar" / "outputs" / "orchestrator_runs"

        # Find latest .log
        log_files = list(run_dir.glob("*.log"))
        if not log_files:
            return jsonify({"logs": ["No logs found."]})

        latest_log = max(log_files, key=lambda f: f.stat().st_mtime)

        # Read last N lines
        content = latest_log.read_text(encoding="utf-8", errors="ignore")
        lines = content.splitlines()[-50:]  # Last 50 lines

        return jsonify({"logs": lines})
    except Exception as e:
        return jsonify({"logs": [f"Error reading logs: {str(e)}"]})


@adapter_bp.route("/scholar/api-key", methods=["POST"])
def update_api_key():
    """Update API Config."""
    data = request.json
    # TODO: Implement updating api_config.json safely
    return jsonify({"success": True})


@adapter_bp.route("/chat/<session_id>", methods=["POST"])
def chat_message(session_id):
    """
    Mimics: app.post("/api/chat/:sessionId")
    Connects to the real Tutor Engine (RAG + OpenRouter).
    """
    try:
        # Import Tutor Engine components dynamically
        import sys
        from pathlib import Path

        brain_dir = Path(__file__).resolve().parent.parent
        if str(brain_dir) not in sys.path:
            sys.path.append(str(brain_dir))

        from brain.tutor_engine import process_tutor_turn, log_tutor_turn
        from brain.tutor_api_types import TutorQueryV1, TutorSourceSelector

        data = request.json
        user_message = data.get("content")

        if not user_message:
            return jsonify({"error": "Message content required"}), 400

        # Construct Query Object
        query = TutorQueryV1(
            user_id="user",
            session_id=str(session_id),
            course_id=None,
            topic_id=None,
            mode="Core",
            question=user_message,
            plan_snapshot_json="{}",
            sources=TutorSourceSelector(),
        )

        # Process with Tutor Engine
        response = process_tutor_turn(query)

        # Log the turn
        log_tutor_turn(query, response)

        # Return in format expected by React frontend
        return jsonify(
            {
                "id": int(datetime.now().timestamp()),
                "sender": "ai",
                "content": response.answer,
                "timestamp": datetime.now().isoformat(),
                "unverified": response.unverified,
                "citations": [asdict(c) for c in response.citations]
                if response.citations
                else [],
            }
        )

    except Exception as e:
        print(f"Tutor Engine Error: {e}")
        import traceback

        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


# ==============================================================================
# GOOGLE CALENDAR
# ==============================================================================


@adapter_bp.route("/google/status", methods=["GET"])
def google_status():
    """
    Mimics: app.get("/api/google/status")
    Checks if backend is connected to Google Calendar.
    """
    from brain.dashboard import gcal

    status = gcal.check_auth_status()
    # status = { connected: bool, email: str, error: str }

    # Frontend expects: configured, connected, hasClientId...
    is_connected = status.get("connected", False)
    error_msg = status.get("error", "")

    # Simple heuristic: if specific config error, then not configured
    is_configured = "configured" not in error_msg.lower()

    return jsonify(
        {
            "configured": is_configured,
            "connected": is_connected,
            "hasClientId": is_configured,
            "hasClientSecret": is_configured,
            "email": status.get("email"),
        }
    )


@adapter_bp.route("/google/auth", methods=["GET"])
def google_auth_url():
    """
    Mimics: app.get("/api/google/auth")
    Returns the OAuth URL to start the flow.
    """
    from brain.dashboard import gcal

    url, state_or_msg = gcal.get_auth_url()
    if not url:
        return jsonify({"error": state_or_msg}), 500

    return jsonify({"authUrl": url})


@adapter_bp.route("/gcal/oauth/callback", methods=["GET"])
def google_callback():
    """
    The redirect target for Google OAuth.
    Exchanges code for token, then redirects user back to dashboard.
    """
    from flask import redirect
    from brain.dashboard import gcal

    code = request.args.get("code")
    if not code:
        return "Missing code", 400

    success, msg = gcal.complete_oauth(code)

    if success:
        # Trigger an initial sync in background?
        # For now, just redirect home
        return redirect("/")

    return f"Google Auth Failed: {msg}", 400


@adapter_bp.route("/google/disconnect", methods=["POST"])
def google_disconnect():
    """
    Mimics: app.post("/api/google/disconnect")
    """
    from brain.dashboard import gcal

    gcal.revoke_auth()
    return jsonify({"success": True})


# ==============================================================================
# GOOGLE DATA PROXIES (Direct Fetch)
# ==============================================================================


@adapter_bp.route("/google-calendar/calendars", methods=["GET"])
def get_google_calendars():
    from brain.dashboard import gcal

    calendars, error = gcal.fetch_calendar_list()
    if error:
        return jsonify({"error": error}), 500

    # Inject backgroundColor if missing (gcal.py doesn't always expose it raw)
    # But usually 'backgroundColor' is in the calendar resource
    # Frontend expects: id, summary, backgroundColor
    return jsonify(calendars)


@adapter_bp.route("/google-calendar/events", methods=["GET"])
def get_google_events():
    from brain.dashboard import gcal
    from datetime import datetime

    # Frontend sends timeMin, timeMax
    # gcal.fetch_calendar_events currently uses hardcoded logic or list of IDs
    # We need to fetch all selected calendars.
    # For now, let's fetch 'primary' or all configured.

    config = gcal.load_gcal_config() or {}
    # Force visibility of ALL calendars for the frontend
    config["sync_all_calendars"] = True

    calendars, _ = gcal.fetch_calendar_list()
    selected_ids, _, _, calendar_meta = gcal.resolve_calendar_selection(
        config, calendars
    )

    # Build a color map
    calendar_colors = {c["id"]: c.get("backgroundColor", "#ef4444") for c in calendars}

    time_min = request.args.get("timeMin")
    time_max = request.args.get("timeMax")

    events, error = gcal.fetch_calendar_events(
        selected_ids, calendar_meta, time_min=time_min, time_max=time_max
    )
    if error:
        return jsonify({"error": error}), 500

    # Enrich events for frontend
    enriched_events = []
    for event in events:
        cal_id = event.get("_calendar_id")
        # Map fields for frontend
        event["calendarId"] = cal_id
        event["calendarSummary"] = event.get("_calendar_name")
        event["calendarColor"] = calendar_colors.get(cal_id)
        enriched_events.append(event)

    enriched_events = []
    for event in events:
        cal_id = event.get("_calendar_id")
        # Map fields for frontend
        event["calendarId"] = cal_id
        event["calendarSummary"] = event.get("_calendar_name")
        event["calendarColor"] = calendar_colors.get(cal_id)
        enriched_events.append(event)

    return jsonify(enriched_events)


@adapter_bp.route("/google-calendar/events", methods=["POST"])
def create_google_event():
    from brain.dashboard import gcal

    data = request.json
    calendar_id = data.get("calendarId")
    if not calendar_id:
        return jsonify({"error": "Missing calendarId"}), 400

    # Construct "local_event" format expected by gcal.upsert_gcal_event
    local_event = {
        "title": data.get("title"),
        "date": data.get("date"),  # ISO string
        "raw_text": data.get("description", ""),
        # Additional parsing might be needed if date processing is complex
    }

    service = gcal.get_calendar_service()
    if not service:
        return jsonify({"error": "Not authenticated"}), 401

    # We might need calendar metadata (timezone)
    config = gcal.load_gcal_config()
    calendars, _ = gcal.fetch_calendar_list()
    cal_meta = next((c for c in calendars if c["id"] == calendar_id), {})
    timezone = cal_meta.get("timeZone", "UTC")

    # We need to adapt the payload because upsert_gcal_event expects
    # a "local_event" structure which is DB-centric.
    # Alternatively, we can just call service directly here for simplicity if gcal.py is too coupled to DB.
    # However, let's try to reuse or adapt.
    # gcal.upsert_gcal_event calls build_gcal_event_payload

    # Let's do a direct insert to avoid DB coupling complexity for now
    # Or cleaner: update gcal.py to expose a clean insert function.
    # For speed, I'll implement a direct service call here mimicking gcal logic.

    try:
        from datetime import datetime, timedelta

        start_dt = datetime.fromisoformat(data["date"].replace("Z", "+00:00"))
        if data.get("allDay"):
            end_dt = (
                datetime.fromisoformat(data.get("endDate").replace("Z", "+00:00"))
                if data.get("endDate")
                else start_dt + timedelta(days=1)
            )
            start = {"date": start_dt.date().isoformat()}
            end = {"date": end_dt.date().isoformat()}
        else:
            end_dt = (
                datetime.fromisoformat(data.get("endDate").replace("Z", "+00:00"))
                if data.get("endDate")
                else start_dt + timedelta(hours=1)
            )
            start = {"dateTime": start_dt.isoformat(), "timeZone": timezone}
            end = {"dateTime": end_dt.isoformat(), "timeZone": timezone}

        body = {
            "summary": data.get("title", "Untitled"),
            "start": start,
            "end": end,
            "description": data.get("description", ""),
        }

        # Recurrence
        if data.get("recurrence"):
            # Ensure it's a list
            rrules = data["recurrence"]
            if isinstance(rrules, str):
                rrules = [rrules]
            body["recurrence"] = rrules

        event = service.events().insert(calendarId=calendar_id, body=body).execute()
        return jsonify(event)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/google-calendar/events/<event_id>", methods=["PATCH"])
@adapter_bp.route("/google-calendar/events/<event_id>", methods=["PATCH"])
def update_google_event(event_id):
    from brain.dashboard import gcal

    data = request.json
    calendar_id = data.get("calendarId")
    if not calendar_id:
        return jsonify({"error": "Missing calendarId"}), 400

    service = gcal.get_calendar_service()
    if not service:
        return jsonify({"error": "Not authenticated"}), 401

    try:
        # 1. Verify Access Role
        calendars, _ = gcal.fetch_calendar_list()
        target_cal = next((c for c in calendars if c["id"] == calendar_id), None)

        if not target_cal:
            # If not found in list, we might not have access or it's hidden
            # Try to proceed? Or fail safe?
            # Let's try to fetch it specifically if missing, or assume we can't write if we can't see it.
            # Strict permission check:
            pass
        else:
            role = target_cal.get("accessRole", "reader")
            if role not in ["owner", "writer"]:
                return jsonify(
                    {
                        "error": f"Permission denied: You have '{role}' access to this calendar."
                    }
                ), 403

        # 2. Fetch existing event (Crucial for patches to minimal fields)
        try:
            existing_event = (
                service.events().get(calendarId=calendar_id, eventId=event_id).execute()
            )
        except Exception as e:
            return jsonify({"error": f"Event not found: {str(e)}"}), 404

        # 3. Prepare Update Body
        body = {}

        # Basic fields
        if "summary" in data:
            body["summary"] = data["summary"]
        elif "title" in data:
            body["summary"] = data["title"]

        if "description" in data:
            body["description"] = data["description"]

        if "location" in data:
            body["location"] = data["location"]

        # Recurrence
        if "recurrence" in data:
            rrules = data["recurrence"]
            if isinstance(rrules, str):
                rrules = [rrules]
            body["recurrence"] = rrules

        # Date Logic
        # Frontend might send 'start'/'end' objects OR 'date'/'startDate' strings.
        # We prioritize 'start'/'end' objects if valid.

        if "start" in data and isinstance(data["start"], dict):
            body["start"] = data["start"]

        if "end" in data and isinstance(data["end"], dict):
            body["end"] = data["end"]

        # Fallback: Constructed date logic if objects missing but flat fields present
        # (This matches the Agent task description "If allDay or date has no T...")
        if "start" not in body and ("date" in data or "allDay" in data):
            is_all_day = data.get("allDay", False)
            date_val = data.get("date")
            end_date_val = data.get("endDate")

            if not date_val:
                # Keep existing start if not updating date?
                # If we are here, we probably aren't updating date.
                pass
            else:
                if is_all_day or "T" not in date_val:
                    # All Day
                    start_d = date_val.split("T")[0]
                    if end_date_val:
                        end_d = end_date_val.split("T")[0]
                    else:
                        # Default 1 day
                        from datetime import datetime, timedelta

                        dt = datetime.strptime(start_d, "%Y-%m-%d")
                        end_d = (dt + timedelta(days=1)).strftime("%Y-%m-%d")

                    body["start"] = {"date": start_d}
                    body["end"] = {"date": end_d}
                else:
                    # Timed
                    body["start"] = {"dateTime": date_val}
                    # If end missing, default
                    if end_date_val:
                        body["end"] = {"dateTime": end_date_val}
                    else:
                        # Fallback to existing end duration or +1h
                        # Easier: +1h
                        from datetime import datetime, timedelta

                        dt = datetime.fromisoformat(date_val.replace("Z", "+00:00"))
                        end_dt = dt + timedelta(hours=1)
                        body["end"] = {"dateTime": end_dt.isoformat()}

        # 4. Patch
        updated_event = (
            service.events()
            .patch(calendarId=calendar_id, eventId=event_id, body=body)
            .execute()
        )

        return jsonify(updated_event)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/google-calendar/events/<event_id>", methods=["DELETE"])
def delete_google_event(event_id):
    from brain.dashboard import gcal

    calendar_id = request.args.get("calendarId")
    if not calendar_id:
        return jsonify({"error": "Missing calendarId parameter"}), 400

    service = gcal.get_calendar_service()
    if not service:
        return jsonify({"error": "Not authenticated"}), 401

    try:
        service.events().delete(calendarId=calendar_id, eventId=event_id).execute()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/google-calendar/clear", methods=["POST"])
def clear_calendars():
    from brain.dashboard import gcal

    data = request.json
    calendar_ids = data.get("calendarIds", [])

    if not calendar_ids:
        return jsonify({"error": "No calendarIds provided"}), 400

    service = gcal.get_calendar_service()
    if not service:
        return jsonify({"error": "Not authenticated"}), 401

    deleted_count = 0
    errors = []

    for cal_id in calendar_ids:
        try:
            # Fetch all events (singleEvents=False to get series parent, but 'delete' works on ID)
            # If we want to clear everything, iterating list is best.
            pageToken = None
            while True:
                events_res = (
                    service.events()
                    .list(calendarId=cal_id, pageToken=pageToken)
                    .execute()
                )
                items = events_res.get("items", [])

                for item in items:
                    try:
                        service.events().delete(
                            calendarId=cal_id, eventId=item["id"]
                        ).execute()
                        deleted_count += 1
                    except Exception as ex:
                        pass  # Best effort

                pageToken = events_res.get("nextPageToken")
                if not pageToken:
                    break
        except Exception as e:
            errors.append(f"{cal_id}: {str(e)}")

    return jsonify({"success": True, "deletedEvents": deleted_count, "errors": errors})


@adapter_bp.route("/google-tasks", methods=["GET"])
def get_google_tasks():
    from brain.dashboard import gcal

    task_lists, error = gcal.fetch_task_lists()
    if error:
        return jsonify({"error": error}), 500

    all_tasks = []

    # Iterate over ALL lists found
    for tl in task_lists:
        t_list, err = gcal.fetch_tasks_from_list(tl["id"])
        if err:
            continue  # specific list fail shouldn't crash all

        for t in t_list:
            t["listId"] = tl["id"]
            t["listTitle"] = tl["title"]
            all_tasks.append(t)

    # Include list metadata so frontend knows available lists
    # We can handle this by a separate endpoint or embedding it.
    # Frontend requirement: "Add list selector".
    # I'll embed `lists` in the response or trust frontend calls another endpoint?
    # Better: return { tasks: [...], lists: [...] } ?
    # Standard REST: GET /tasks returns tasks. GET /lists returns lists.
    # Current adapter structure returns array of tasks.
    # I'll stick to array of tasks. Frontend can deduce lists from the tasks or we add a separate endpoint /google-tasks/lists.
    # User didn't ask for /lists endpoint explicitly but "Add list selector" implies we need available lists.
    # I'll just return the tasks. Frontend can unique() the listIds or I'll add a /lists endpoint if needed.
    # Wait, if a list is empty, frontend won't know it exists.
    # I'll modify returning structure or add /lists endpoint. I'll add /google-tasks/lists.

    return jsonify(all_tasks)


@adapter_bp.route("/google-tasks/lists", methods=["GET"])
def get_google_task_lists():
    from brain.dashboard import gcal

    task_lists, error = gcal.fetch_task_lists()
    if error:
        return jsonify({"error": error}), 500

    return jsonify(task_lists)


@adapter_bp.route("/google-tasks", methods=["POST"])
def create_google_task_endpoint():
    from brain.dashboard import gcal

    data = request.json
    list_id = data.get("listId")
    if not list_id:
        return jsonify({"error": "Missing listId"}), 400

    # Construct body
    body = {
        "title": data.get("title"),
        "notes": data.get("notes"),
        "status": "completed" if data.get("completed") else "needsAction",
    }
    if data.get("due"):
        # due is date-only string RFC 3339 timestamp but API says: "DueDate (as an RFC 3339 timestamp) ... optional time portion is discarded".
        # We accept ISO string.
        body["due"] = data.get("due")

    result, error = gcal.create_google_task(list_id, body)
    if error:
        return jsonify({"error": error}), 500
    return jsonify(result)


@adapter_bp.route("/google-tasks/<task_id>", methods=["PATCH"])
def patch_google_task_endpoint(task_id):
    from brain.dashboard import gcal

    data = request.json
    list_id = data.get("listId")
    if not list_id:
        return jsonify({"error": "Missing listId"}), 400

    body = {}
    if "title" in data:
        body["title"] = data["title"]
    if "notes" in data:
        body["notes"] = data["notes"]
    if "status" in data:
        body["status"] = data["status"]
    if "due" in data:
        body["due"] = data["due"]
    if "completed" in data:
        body["status"] = "completed" if data["completed"] else "needsAction"
        # If un-completing, we might need to clear 'completed' date field? API handles status logic.
        if not data["completed"]:
            body["completed"] = None

    result, error = gcal.patch_google_task(list_id, task_id, body)
    if error:
        return jsonify({"error": error}), 500
    return jsonify(result)


@adapter_bp.route("/google-tasks/<task_id>", methods=["DELETE"])
def delete_google_task_endpoint(task_id):
    from brain.dashboard import gcal

    list_id = request.args.get("listId")
    if not list_id:
        return jsonify({"error": "Missing listId"}), 400

    success, error = gcal.delete_google_task(list_id, task_id)
    if not success:
        return jsonify({"error": error}), 500
    return jsonify({"success": True})


@adapter_bp.route("/google-tasks/<task_id>/move", methods=["POST"])
def move_google_task_endpoint(task_id):
    from brain.dashboard import gcal

    data = request.json
    list_id = data.get("listId")
    dest_list_id = data.get("destinationListId")
    previous = data.get("previousTaskId")
    parent = data.get("parentTaskId")

    if not list_id:
        return jsonify({"error": "Missing listId"}), 400

    # Cross-list Move
    if dest_list_id and dest_list_id != list_id:
        # 1. Fetch source task
        # We assume we don't have full body here, need to fetch it?
        # Or frontend sends body? Frontend logic "reorder" might not send full body.
        # Safest is fetch.
        # But gcal helper fetch_tasks_from_list returns list.
        # I need `get_task`. I didn't add `get_task` helper.
        # I'll rely on frontend or add helper.
        # I'll assume frontend sends title/notes/status/due if resizing complexity.
        # For now, I'll attempt to fetch by filtering the list (slow) or modifying gcal.py.
        # I'll modify gcal.py? No, I'll iterate list.
        # Wait, I can just use `get_tasks_service` here directly if needed.
        service = gcal.get_tasks_service()
        if not service:
            return jsonify({"error": "Auth"}), 401

        try:
            task = service.tasks().get(tasklist=list_id, task=task_id).execute()
        except:
            return jsonify({"error": "Task not found"}), 404

        # 2. Insert into dest
        new_body = {
            "title": task.get("title"),
            "notes": task.get("notes"),
            "status": task.get("status"),
            "due": task.get("due"),
        }
        # Insert with previous/parent if supported? Insert supports 'previous' and 'parent' params!
        # insert(..., previous=previous, parent=parent)
        insert_kwargs = {"tasklist": dest_list_id, "body": new_body}
        if previous:
            insert_kwargs["previous"] = previous
        if parent:
            insert_kwargs["parent"] = parent

        try:
            new_task = service.tasks().insert(**insert_kwargs).execute()
            # 3. Delete from source
            service.tasks().delete(tasklist=list_id, task=task_id).execute()
            return jsonify(new_task)
        except Exception as e:
            return jsonify({"error": str(e)}), 500

    # Same-list Reorder
    result, error = gcal.move_google_task(list_id, task_id, previous, parent)
    if error:
        return jsonify({"error": error}), 500
    return jsonify(result)


# ==============================================================================
# CALENDAR ASSISTANT (Copilot w/ Codex + Undo) - v9.3
# ==============================================================================


@adapter_bp.route("/calendar/assistant", methods=["POST"])
def calendar_assistant_endpoint():
    """
    Chat with Calendar Assistant using direct OpenAI API.
    Stateless, simple tool calling for calendar operations.
    """
    data = request.json
    messages = data.get("messages", [])

    if not messages:
        return jsonify({"error": "No messages provided"}), 400

    last_user_msg = messages[-1]["content"]

    try:
        # Import the simple calendar assistant module
        from dashboard.calendar_assistant import run_calendar_assistant

        # Run the calendar assistant
        result = run_calendar_assistant(last_user_msg)

        if result.get("success"):
            return jsonify(
                {
                    "response": result["response"],
                    "can_undo": False,  # TODO: Add undo support
                }
            )
        else:
            return jsonify({"error": result.get("error", "Unknown error")}), 500

    except ImportError as e:
        print(f"[Calendar Assistant] Import failed: {e}")
        return jsonify(
            {
                "response": "Calendar assistant is temporarily unavailable. Please try again.",
                "error": str(e),
            }
        ), 503
    except Exception as e:
        import traceback

        print(f"[Calendar Assistant] Error: {e}")
        traceback.print_exc()
        return jsonify({"error": f"Assistant error: {str(e)}"}), 500


@adapter_bp.route("/calendar/assistant/undo", methods=["POST"])
def undo_calendar_action_endpoint():
    """Revert the last action in the ledger."""
    from brain.db_setup import get_connection
    from brain.dashboard import gcal

    try:
        conn = get_connection()
        cur = conn.cursor()
        # Get last action
        cur.execute(
            "SELECT id, action_type, target_id, pre_state, post_state FROM calendar_action_ledger ORDER BY id DESC LIMIT 1"
        )
        row = cur.fetchone()

        if not row:
            conn.close()
            return jsonify({"error": "Nothing to undo"}), 400

        lid, action, tid, pre_json, post_json = row
        pre_state = json.loads(pre_json) if pre_json else None

        # Undo Logic
        if action == "create_event":
            # Inverse: Delete
            gcal.delete_event("primary", tid)

        elif action == "delete_event":
            # Inverse: Re-create (using pre_state)
            if pre_state:
                # Remove ID to let Google assign new one? Or reuse?
                # Insert ignores ID usually. Import might allow it.
                # Just insert as new.
                body = {
                    "summary": pre_state.get("summary"),
                    "description": pre_state.get("description"),
                    "start": pre_state.get("start"),
                    "end": pre_state.get("end"),
                }
                gcal.create_event("primary", body)

        elif action == "create_task":
            # Inverse: Delete (need list_id - stored? No. Need to parse from pre/post or store list_id in ledger?)
            # I didn't store list_id in ledger column specifically.
            # But post_state might have it? (Not standard Google Task resource field?)
            # Actually, Google Task resource doesn't always have 'listId'.
            # Fail: I need list_id to delete.
            # Assume I can find it or I should have stored context in description or separate col.
            # For now, simplistic undo might fail for tasks if list_id missing.
            # I'll try to find it from post_state json if I enriched it?
            # In create_task above: I did NOT enrich post_state with listId.
            # FIXME: Storing 'selfLink' might help?
            pass  # TODO: Fix Task Undo context

        elif action == "delete_task":
            # Inverse: Re-create
            pass

        # Delete ledger entry
        cur.execute("DELETE FROM calendar_action_ledger WHERE id = ?", (lid,))
        conn.commit()
        conn.close()

        return jsonify({"success": True, "message": f"Undid {action}"})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ==============================================================================
# NOTES (Quick Notes / Scratchpad)
# ==============================================================================


@adapter_bp.route("/notes", methods=["GET"])
def get_notes():
    """Get all quick notes."""
    try:
        conn = get_connection()
        cur = conn.cursor()
        cur.execute(
            "SELECT id, title, content, position, created_at, updated_at FROM quick_notes ORDER BY position ASC, created_at DESC"
        )
        rows = cur.fetchall()

        notes = []
        for r in rows:
            notes.append(
                {
                    "id": r[0],
                    "title": r[1],
                    "content": r[2],
                    "position": r[3],
                    "createdAt": r[4],
                    "updatedAt": r[5],
                }
            )

        conn.close()
        return jsonify(notes)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/notes", methods=["POST"])
def create_note():
    """Create a new note."""
    data = request.json
    from datetime import datetime

    try:
        conn = get_connection()
        cur = conn.cursor()
        now = datetime.now().isoformat()

        # Get max position
        cur.execute("SELECT MAX(position) FROM quick_notes")
        max_pos = cur.fetchone()[0] or 0

        cur.execute(
            """
            INSERT INTO quick_notes (title, content, position, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?)
        """,
            (data.get("title", ""), data.get("content", ""), max_pos + 1, now, now),
        )

        new_id = cur.lastrowid
        conn.commit()
        conn.close()

        return jsonify(
            {
                "id": new_id,
                "title": data.get("title", ""),
                "content": data.get("content", ""),
                "position": max_pos + 1,
                "createdAt": now,
                "updatedAt": now,
            }
        ), 201
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/notes/<int:note_id>", methods=["PATCH"])
def update_note(note_id):
    """Update a note."""
    data = request.json
    from datetime import datetime

    try:
        conn = get_connection()
        cur = conn.cursor()

        fields = ["updated_at = ?"]
        values = [datetime.now().isoformat()]

        if "title" in data:
            fields.append("title = ?")
            values.append(data["title"])
        if "content" in data:
            fields.append("content = ?")
            values.append(data["content"])
        if "position" in data:
            fields.append("position = ?")
            values.append(data["position"])

        values.append(note_id)

        cur.execute(
            f"UPDATE quick_notes SET {', '.join(fields)} WHERE id = ?", tuple(values)
        )
        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/notes/<int:note_id>", methods=["DELETE"])
def delete_note(note_id):
    """Delete a note."""
    try:
        conn = get_connection()
        cur = conn.cursor()
        cur.execute("DELETE FROM quick_notes WHERE id = ?", (note_id,))
        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@adapter_bp.route("/notes/reorder", methods=["POST"])
def reorder_notes():
    """Update positions for a batch of notes."""
    # Expects { "notes": [ {id: 1, position: 0}, ... ] } or "updates"
    data = request.json
    try:
        conn = get_connection()
        cur = conn.cursor()

        items = data.get("notes", []) or data.get("updates", [])
        for item in items:
            cur.execute(
                "UPDATE quick_notes SET position = ? WHERE id = ?",
                (item["position"], item["id"]),
            )

        conn.commit()
        conn.close()
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ==============================================================================
# CHAT HISTORY
# ==============================================================================


@adapter_bp.route("/chat/<session_id>", methods=["GET"])
def get_chat_history(session_id):
    """Get chat history for a tutor session."""
    try:
        conn = get_connection()
        cur = conn.cursor()

        # Simple retrieval from tutor_turns
        cur.execute(
            """
            SELECT turn_number, question, answer, citations_json, created_at 
            FROM tutor_turns 
            WHERE session_id = ? 
            ORDER BY turn_number ASC
        """,
            (session_id,),
        )

        rows = cur.fetchall()
        history = []
        for r in rows:
            history.append({"role": "user", "content": r[1], "timestamp": r[4]})
            history.append(
                {
                    "role": "assistant",
                    "content": r[2],
                    "citations": json.loads(r[3]) if r[3] else [],
                    "timestamp": r[4],
                }
            )

        conn.close()
        return jsonify(history)
    except Exception as e:
        return jsonify({"error": str(e)}), 500
