
import os
import re
import json
import sqlite3
import subprocess
import tempfile
from pathlib import Path
from datetime import datetime, timedelta
from flask import (
    Blueprint,
    render_template,
    jsonify,
    request,
    send_file,
    Response,
)
from werkzeug.utils import secure_filename

from config import (
    DATA_DIR,
    STUDY_RAG_DIR,
    SESSION_LOGS_DIR,
    SCORE_MIN,
    SCORE_MAX,
    STALE_DAYS,
    FRESH_DAYS,
    WEAK_THRESHOLD,
    STRONG_THRESHOLD,
)
from db_setup import DB_PATH, init_database, get_connection
# Use import directly for items in brain/ folder (root of execution)
from ingest_session import parse_session_log, validate_session_data, insert_session
from generate_resume import generate_resume
from tutor_api_types import TutorQueryV1, TutorSourceSelector, TutorTurnResponse
from tutor_engine import process_tutor_turn, log_tutor_turn, create_card_draft_from_turn
from import_syllabus import upsert_course, import_events
from rag_notes import (
    ingest_document,
    ingest_url_document,
    sync_folder_to_rag,
    sync_runtime_catalog,
)

# Dashboard modules
from dashboard.utils import allowed_file, load_api_config, save_api_config
from dashboard.stats import build_stats
from dashboard.scholar import (
    build_scholar_stats, 
    generate_ai_answer, 
    run_scholar_orchestrator,
    MAX_CONTEXT_CHARS,
)
from dashboard.syllabus import fetch_all_courses_and_events, attach_event_analytics
from dashboard.calendar import get_calendar_data
from dashboard.cli import get_all_sessions

dashboard_bp = Blueprint('dashboard', __name__)

# Ensure directories exist (run once at module level is okay, or in app factory)
# We can do it here just to be safe if blueprint is imported
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(SESSION_LOGS_DIR, exist_ok=True)

PROJECT_FILES_DIR = Path(DATA_DIR) / "project_files"
os.makedirs(PROJECT_FILES_DIR, exist_ok=True)

STUDY_RAG_PATH = Path(STUDY_RAG_DIR)
os.makedirs(STUDY_RAG_PATH, exist_ok=True)


def insert_session_data(data):
    """
    Validate and insert using the v9.1 ingest pipeline.
    Returns (ok: bool, message: str).
    """
    is_valid, error = validate_session_data(data)
    if not is_valid:
        return False, f"Validation failed: {error}"
    return insert_session(data)


@dashboard_bp.route("/")
def index():
    return render_template('dashboard.html')


@dashboard_bp.route("/favicon.ico")
def favicon():
    return "", 204


@dashboard_bp.route("/api/stats")
def api_stats():
    return jsonify(build_stats())


@dashboard_bp.route("/api/scholar")
def api_scholar():
    return jsonify(build_scholar_stats())


@dashboard_bp.route("/api/scholar/api-key", methods=["GET", "POST"])
def api_scholar_api_key():
    if request.method == "GET":
        config = load_api_config()
        api_provider = config.get("api_provider", "openrouter")
        if api_provider == "openrouter":
            api_key = config.get("openrouter_api_key", "")
        else:
            api_key = config.get("openai_api_key", "")
        
        return jsonify({
            "ok": True,
            "has_key": bool(api_key),
            "key_preview": f"{api_key[:7]}..." if api_key else None,
            "api_provider": api_provider,
            "model": config.get("model", "openrouter/auto"),
        })
    else:
        try:
            data = request.get_json()
            if not data or "api_key" not in data:
                return jsonify({"ok": False, "message": "Missing 'api_key' field"}), 400
            
            api_key = data["api_key"].strip()
            if not api_key:
                return jsonify({"ok": False, "message": "API key cannot be empty"}), 400
            
            config = load_api_config()
            api_provider = data.get("api_provider", "openrouter")
            model = data.get("model", "openrouter/auto")
            
            if api_provider == "openrouter":
                config["openrouter_api_key"] = api_key
            else:
                config["openai_api_key"] = api_key
            
            config["api_provider"] = api_provider
            config["model"] = model
            save_api_config(config)
            
            return jsonify({
                "ok": True,
                "message": f"API key saved successfully ({api_provider})",
                "key_preview": f"{api_key[:7]}...",
                "api_provider": api_provider,
                "model": model,
            })
        except Exception as e:
            return jsonify({"ok": False, "message": f"Error saving API key: {e}"}), 500


@dashboard_bp.route("/api/scholar/questions/clarify", methods=["POST"])
def api_scholar_clarify_question():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"ok": False, "message": "Missing JSON body"}), 400
            
        # Support "messages" (chat history) or single "clarifying_question"
        scholar_question = data.get("scholar_question", "")
        
        history = data.get("messages", [])
        user_message = ""
        if "clarifying_question" in data:
            user_message = data["clarifying_question"]
        elif history:
            user_message = history[-1]["content"] if history[-1]["role"] == "user" else ""
            
        if not user_message:
             return jsonify({"ok": False, "message": "No question provided"}), 400
        
        # Load Scholar context
        repo_root = Path(__file__).parent.parent.parent.resolve()
        
        # Build system context
        context_parts = []
        context_parts.append(f"Primary Scholar Question being discussed: {scholar_question}")
        
        # Load docs
        for doc_name in ["PEIRRO.md", "KWIK.md", "M6-wrap.md"]:
             doc_path = repo_root / "sop" / "gpt-knowledge" / doc_name
             if doc_path.exists():
                 try:
                     context_parts.append(f"\n--- {doc_name} ---\n{doc_path.read_text(encoding='utf-8')[:1000]}...")
                 except: pass

        system_context = "\n".join(context_parts)
        
        # If using history, pass it to generate_ai_answer if it supported list, 
        # but generate_ai_answer takes (question, context).
        # We'll prepend history to context or modify generate_ai_answer.
        # For now, let's construct a prompt with history.
        
        full_prompt = user_message
        if history:
            # Limit history to last 10 messages and truncate each message
            history_limited = history[-11:-1] if len(history) > 10 else history[:-1]
            history_text = "\\n".join([f"{m['role'].upper()}: {m['content'][:500]}" for m in history_limited])
            if history_text:
                full_prompt = f"Conversation History:\\n{history_text}\\n\\nCurrent Question: {user_message}"
        
        # Truncate system_context if too long
        if len(system_context) > MAX_CONTEXT_CHARS:
            system_context = system_context[:MAX_CONTEXT_CHARS] + "\\n[... truncated ...]"

        answer, error = generate_ai_answer(full_prompt, system_context)
        
        if error:
            return jsonify({"ok": False, "message": error}), 400
        
        return jsonify({
            "ok": True,
            "clarification": answer,
            "scholar_question": scholar_question,
        })
    except Exception as e:
        return jsonify({"ok": False, "message": f"Error generating clarification: {e}"}), 500


@dashboard_bp.route("/api/scholar/questions/generate", methods=["POST"])
def api_scholar_generate_answer():
    try:
        data = request.get_json()
        if not data:
             return jsonify({"ok": False, "message": "Missing JSON body"}), 400

        # Handle "messages" list for chat mode
        messages = data.get("messages", [])
        question = data.get("question", "")
        
        # If messages provided but no question, assume last user message is question
        if messages and not question:
            last_msg = messages[-1]
            if last_msg["role"] == "user":
                question = last_msg["content"]
        
        if not question:
            return jsonify({"ok": False, "message": "Missing 'question' or 'messages' field"}), 400
        
        context = data.get("context", "")
        
        # Load Scholar context
        repo_root = Path(__file__).parent.parent.parent.resolve()
        peirro_doc = repo_root / "sop" / "gpt-knowledge" / "PEIRRO.md"
        kwik_doc = repo_root / "sop" / "gpt-knowledge" / "KWIK.md"
        
        additional_context = ""
        if peirro_doc.exists():
            try:
                peirro_content = peirro_doc.read_text(encoding="utf-8")[:500]
                additional_context += f"\nPEIRRO Framework: {peirro_content}\n"
            except: pass
        if kwik_doc.exists():
            try:
                kwik_content = kwik_doc.read_text(encoding="utf-8")[:500]
                additional_context += f"\nKWIK Framework: {kwik_content}\n"
            except: pass
        
        full_context = context + additional_context
        
        # Append history to context if available (limit to last 10 messages to avoid overflow)
        if messages:
             history_msgs = messages[:-1] if messages[-1].get("role") == "user" else messages
             # Limit history to avoid token overflow
             history_msgs = history_msgs[-10:] if len(history_msgs) > 10 else history_msgs
             history_str = "\\n".join([f"{m['role'].upper()}: {m['content'][:500]}" for m in history_msgs])
             if history_str:
                 full_context += f"\\n\\nPrevious Conversation:\\n{history_str}"
        
        # Truncate total context if too long
        if len(full_context) > MAX_CONTEXT_CHARS:
            full_context = full_context[:MAX_CONTEXT_CHARS] + "\\n[... truncated ...]"
        
        api_key_override = data.get("api_key")
        api_provider_override = data.get("api_provider")
        model_override = data.get("model")

        answer, error = generate_ai_answer(
            question,
            full_context,
            api_key_override=api_key_override,
            api_provider_override=api_provider_override,
            model_override=model_override,
        )
        
        if error:
            return jsonify({"ok": False, "message": error}), 400
        
        return jsonify({
            "ok": True,
            "answer": answer,
            "question": question,
        })
    except Exception as e:
        return jsonify({"ok": False, "message": f"Error generating answer: {e}"}), 500


@dashboard_bp.route("/api/scholar/questions", methods=["POST"])
def api_scholar_answer_questions():
    """Update the questions_needed file with answers."""
    try:
        data = request.get_json()
        if not data or "answers" not in data:
            return jsonify({"ok": False, "message": "Missing 'answers' array"}), 400
        
        answers = data["answers"]
        repo_root = Path(__file__).parent.parent.parent.resolve()
        orchestrator_runs = repo_root / "scholar" / "outputs" / "orchestrator_runs"
        
        # Find the questions file with unanswered questions (check recent files)
        question_files = sorted(
            orchestrator_runs.glob("questions_needed_*.md"), 
            key=lambda p: p.stat().st_mtime, 
            reverse=True
        )
        
        if not question_files:
            return jsonify({"ok": False, "message": "No questions file found"}), 404
        
        # This logic finds the file to update - simplified from original for brevity but robust enough
        target_questions_file = question_files[0]
        
        # Check if we should use an older file that has unanswered questions
        for q_file in question_files[:3]:
            try:
                content = q_file.read_text(encoding="utf-8").strip()
                if content and content != "(none)":
                    has_unanswered = False
                    if "Q:" in content:
                        lines = content.split("\n")
                        for i, line in enumerate(lines):
                            if line.strip().startswith("Q:"):
                                # Check if next line is A:
                                if i + 1 >= len(lines) or not lines[i+1].strip().startswith("A:") or lines[i+1].replace("A:", "").strip().lower() in ["(pending)", "(none)", ""]:
                                    has_unanswered = True
                                    break
                    elif content and not content.startswith("#"):
                        has_unanswered = True
                    
                    if has_unanswered:
                        target_questions_file = q_file
                        break
            except:
                continue
        
        latest_questions_file = target_questions_file
        current_content = latest_questions_file.read_text(encoding="utf-8")
        
        # Update logic
        if "Q:" in current_content:
            lines = current_content.split("\n")
            answered_content = []
            answer_index = 0
            
            for line in lines:
                if line.strip().startswith("Q:"):
                    answered_content.append(line)
                elif line.strip().startswith("A:"):
                    if answer_index < len(answers) and answers[answer_index]:
                        answered_content.append(f"A: {answers[answer_index]}")
                    else:
                        answered_content.append(line)
                    answer_index += 1
                elif line.strip() and not line.strip().startswith("Q:") and not line.strip().startswith("A:"):
                    answered_content.append(line)
                else:
                    answered_content.append(line)
            
            while answer_index < len(answers):
                if answers[answer_index]:
                    answered_content.append(f"\nQ: (Additional question {answer_index + 1})")
                    answered_content.append(f"A: {answers[answer_index]}")
                answer_index += 1
        else:
            original_questions = [q.strip() for q in current_content.split("\n") 
                                if q.strip() and q.strip() != "(none)" 
                                and not q.strip().startswith("#")]
            
            answered_content = []
            for i, question in enumerate(original_questions):
                clean_question = re.sub(r"^[-*â€¢]\s*", "", question)
                clean_question = re.sub(r"^\d+\.\s*", "", clean_question)
                if clean_question:
                    answered_content.append(f"Q: {clean_question}")
                    if i < len(answers) and answers[i]:
                        answered_content.append(f"A: {answers[i]}")
                    else:
                        answered_content.append(f"A: (pending)")
                    answered_content.append("")
        
        # Atomic write: temp file + rename to prevent corruption on crash
        content = "\n".join(answered_content)
        fd, tmp_path = tempfile.mkstemp(suffix='.md', dir=str(latest_questions_file.parent))
        try:
            with os.fdopen(fd, 'w', encoding='utf-8') as tmp_file:
                tmp_file.write(content)
            # Atomic rename (on same filesystem)
            os.replace(tmp_path, str(latest_questions_file))
        except Exception:
            # Clean up temp file if rename failed
            try:
                os.unlink(tmp_path)
            except Exception:
                pass
            raise
        
        return jsonify({"ok": True, "message": f"Answers saved to {latest_questions_file.name}"})
    except Exception as e:
        return jsonify({"ok": False, "message": f"Error saving answers: {e}"}), 500


@dashboard_bp.route("/api/scholar/run", methods=["POST"])
def api_scholar_run():
    try:
        result = run_scholar_orchestrator()
        return jsonify(result)
    except Exception as e:
        return jsonify({"ok": False, "message": f"Error starting Scholar run: {e}"}), 500


@dashboard_bp.route("/api/scholar/run/status/<run_id>")
def api_scholar_run_status(run_id):
    repo_root = Path(__file__).parent.parent.parent.resolve()
    run_dir = repo_root / "scholar" / "outputs" / "orchestrator_runs"
    
    log_path = run_dir / f"unattended_{run_id}.log"
    final_path = run_dir / f"unattended_final_{run_id}.md"
    pid_path = run_dir / f"unattended_{run_id}.pid"

    def _read_tail_text(path: Path, max_bytes: int = 12000) -> str:
        try:
            with open(path, "rb") as f:
                try:
                    f.seek(0, 2)
                    size = f.tell()
                    f.seek(max(0, size - max_bytes), 0)
                except Exception:
                    pass
                data = f.read()
            return data.decode("utf-8", errors="replace")
        except Exception:
            return ""

    def _pid_is_running(pid: int) -> bool:
        if pid <= 0:
            return False
        if os.name == "nt":
            try:
                # tasklist returns header + row when PID exists
                proc = subprocess.run(
                    ["tasklist", "/FI", f"PID eq {pid}"],
                    capture_output=True,
                    text=True,
                    timeout=3,
                )
                out = (proc.stdout or "")
                # When not found, tasklist prints: "INFO: No tasks are running which match the specified criteria."
                if "No tasks are running" in out:
                    return False
                return re.search(rf"\b{re.escape(str(pid))}\b", out) is not None
            except Exception:
                return False
        try:
            os.kill(pid, 0)
            return True
        except Exception:
            return False
    
    status = {
        "run_id": run_id,
        "running": False,
        "completed": False,
        "has_final": final_path.exists(),
        "log_size": 0,
        "log_tail": "",
        "log_mtime": None,
        "seconds_since_log_update": None,
        "pid": None,
        "pid_stale": False,
        "stalled": False,
        "error": False,
        "error_message": None,
    }

    if log_path.exists():
        try:
            st = log_path.stat()
            status["log_size"] = st.st_size
            status["log_mtime"] = datetime.fromtimestamp(st.st_mtime).isoformat()
            status["seconds_since_log_update"] = int((datetime.now() - datetime.fromtimestamp(st.st_mtime)).total_seconds())
        except Exception:
            pass
        status["log_tail"] = _read_tail_text(log_path, max_bytes=16000)
        
        # Detect error marker in log
        if "===== SCHOLAR RUN ERROR =====" in status["log_tail"]:
            status["error"] = True
            status["running"] = False
            # Extract error message
            try:
                error_start = status["log_tail"].find("===== SCHOLAR RUN ERROR =====")
                error_section = status["log_tail"][error_start:error_start + 500]
                for line in error_section.split("\n"):
                    if line.startswith("Error:"):
                        status["error_message"] = line.replace("Error:", "").strip()
                        break
            except Exception:
                pass

    # Prefer PID-based running detection when available
    if pid_path.exists():
        try:
            pid_txt = pid_path.read_text(encoding="utf-8").strip()
            pid = int(pid_txt)
            status["pid"] = pid
            status["running"] = _pid_is_running(pid)
            if not status["running"]:
                status["pid_stale"] = True
        except Exception:
            pass

    # Completion detection
    tail = status.get("log_tail", "") or ""
    if "===== Scholar Run Completed" in tail or final_path.exists():
        status["completed"] = True
        status["running"] = False
    elif status["log_tail"]:
        # Fallback heuristic when no PID available
        if status["pid"] is None:
            status["running"] = ("ERROR:" not in tail)

    # Stalled signal for UI (no log changes for 10+ minutes while running)
    if status.get("running") and isinstance(status.get("seconds_since_log_update"), int):
        status["stalled"] = status["seconds_since_log_update"] >= 600

    # If PID is stale and we have neither completion marker nor final, this likely exited unexpectedly.
    # Clear stale PID file so UI doesn't keep presenting a "running" affordance.
    if status.get("pid_stale") and (not status.get("completed")) and (not final_path.exists()):
        try:
            pid_path.unlink()
            status["pid_stale"] = False
            status["pid"] = None
        except Exception:
            pass
    
    if final_path.exists():
        status["completed"] = True
        status["running"] = False
        try:
            final_content = final_path.read_text(encoding="utf-8")
            status["final_summary"] = final_content[:500]
        except:
            pass
    
    return jsonify(status)


@dashboard_bp.route("/api/scholar/run/latest-final")
def api_scholar_latest_final():
    repo_root = Path(__file__).parent.parent.parent.resolve()
    run_dir = repo_root / "scholar" / "outputs" / "orchestrator_runs"
    final_files = sorted(run_dir.glob("unattended_final_*.md"), key=lambda p: p.stat().st_mtime, reverse=True)
    if not final_files:
        return jsonify({"ok": False, "message": "No unattended_final file found"}), 404
    path = final_files[0]
    try:
        content = path.read_text(encoding="utf-8")
    except Exception:
        content = path.read_text(encoding="utf-8", errors="replace")
    # Keep payload bounded
    max_chars = 120000
    if len(content) > max_chars:
        content = content[:max_chars] + "\n\n[... truncated ...]"
    return jsonify({
        "ok": True,
        "file": str(path.relative_to(repo_root)).replace("\\", "/"),
        "modified": datetime.fromtimestamp(path.stat().st_mtime).isoformat(),
        "content": content,
    })


@dashboard_bp.route("/api/scholar/run/cancel/<run_id>", methods=["POST"])
def api_scholar_cancel_run(run_id):
    repo_root = Path(__file__).parent.parent.parent.resolve()
    run_dir = repo_root / "scholar" / "outputs" / "orchestrator_runs"
    pid_path = run_dir / f"unattended_{run_id}.pid"
    if not pid_path.exists():
        # Treat as a successful no-op: either the run finished/stopped already, or it was started
        # before PID tracking existed, or the PID file was cleaned up after detecting staleness.
        return jsonify({
            "ok": True,
            "message": "No PID file found for this run (it likely already finished/stopped).",
        })
    try:
        pid = int(pid_path.read_text(encoding="utf-8").strip())
    except Exception:
        return jsonify({"ok": False, "message": "Invalid PID file."}), 400

    try:
        if os.name == "nt":
            subprocess.run(["taskkill", "/PID", str(pid), "/T", "/F"], capture_output=True, text=True, timeout=5)
        else:
            os.kill(pid, 15)
        try:
            pid_path.unlink()
        except Exception:
            pass
        return jsonify({"ok": True, "message": f"Cancel requested for PID {pid}."})
    except Exception as e:
        # If it's already gone, treat cancel as a cleanup.
        try:
            if os.name == "nt":
                proc = subprocess.run(["tasklist", "/FI", f"PID eq {pid}"], capture_output=True, text=True, timeout=3)
                if "No tasks are running" in (proc.stdout or ""):
                    try:
                        pid_path.unlink()
                    except Exception:
                        pass
                    return jsonify({"ok": True, "message": f"PID {pid} is not running; cleared stale PID file."})
        except Exception:
            pass
        return jsonify({"ok": False, "message": f"Failed to cancel run: {e}"}), 500


@dashboard_bp.route("/api/scholar/safe-mode", methods=["POST"])
def api_scholar_safe_mode():
    try:
        data = request.get_json()
        if "safe_mode" not in data:
            return jsonify({"ok": False, "message": "Missing 'safe_mode' field"}), 400
        
        repo_root = Path(__file__).parent.parent.parent.resolve()
        manifest_path = repo_root / "scholar" / "inputs" / "audit_manifest.json"
        
        if not manifest_path.exists():
            return jsonify({"ok": False, "message": "audit_manifest.json not found"}), 404
        
        with open(manifest_path, "r", encoding="utf-8") as f:
            manifest = json.load(f)
        
        manifest["safe_mode"] = bool(data["safe_mode"])
        
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(manifest, f, indent=2)
        
        return jsonify({
            "ok": True, 
            "message": f"Safe mode set to {manifest['safe_mode']}",
            "safe_mode": manifest["safe_mode"]
        })
    except Exception as e:
        return jsonify({"ok": False, "message": f"Error updating safe_mode: {e}"}), 500


@dashboard_bp.route("/api/scholar/execute-via-ai", methods=["POST"])
def api_scholar_execute_via_ai():
    repo_root = Path(__file__).parent.parent.parent.resolve()
    run_dir = repo_root / "scholar" / "outputs" / "orchestrator_runs"
    run_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
    log_path = run_dir / f"unattended_{timestamp}.log"
    final_path = run_dir / f"unattended_final_{timestamp}.md"
    questions_path = run_dir / f"questions_needed_{timestamp}.md"
    
    # Simple preservation check (last file only)
    existing_questions_to_preserve = []
    question_files = sorted(run_dir.glob("questions_needed_*.md"), key=lambda p: p.stat().st_mtime, reverse=True)
    if question_files:
        try:
            content = question_files[0].read_text(encoding="utf-8")
            # Minimal extraction for speed (assumes not empty and not answered)
            if "Q:" not in content and content != "(none)":
                 for line in content.split("\n"):
                    if line.strip() and not line.strip().startswith(("#", "A:")):
                         existing_questions_to_preserve.append(line.strip())
        except:
            pass

    execution_request = run_dir / f"_ai_execution_request_{timestamp}.md"
    request_content = f"# Scholar Execution Request - {timestamp}\n\n"
    request_content += f"Run ID: {timestamp}\n"
    request_content += f"Log: {log_path.relative_to(repo_root)}\n"
    request_content += f"Final: {final_path.relative_to(repo_root)}\n"
    
    execution_request.write_text(request_content, encoding="utf-8")
    
    return jsonify({
        "ok": True,
        "message": "Scholar execution requested via AI assistant",
        "run_id": timestamp,
        "execution_request_file": str(execution_request.relative_to(repo_root)),
    })


@dashboard_bp.route("/api/resume")
def api_resume():
    resume_md = generate_resume()
    if not resume_md:
        return Response("No sessions found.", mimetype="text/plain")
    return Response(resume_md, mimetype="text/plain")


@dashboard_bp.route("/api/resume/download")
def api_resume_download():
    resume_md = generate_resume()
    if not resume_md:
        return Response("No sessions found.", mimetype="text/plain")

    temp_path = Path("session_resume.md")
    temp_path.write_text(resume_md, encoding="utf-8")
    return send_file(
        temp_path,
        as_attachment=True,
        download_name="session_resume.md",
        mimetype="text/markdown",
    )


@dashboard_bp.route("/api/upload", methods=["POST"])
def api_upload():
    if "file" not in request.files:
        return jsonify({"ok": False, "message": "No file part in request."}), 400
    file = request.files["file"]
    if file.filename == "":
        return jsonify({"ok": False, "message": "No selected file."}), 400
    if not allowed_file(file.filename):
        return (
            jsonify({"ok": False, "message": "Unsupported file type. Use .md/.txt."}),
            400,
        )

    filename = secure_filename(file.filename)
    dest_path = Path(SESSION_LOGS_DIR) / filename

    try:
        file.save(dest_path)
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Failed to save file: {exc}"}), 500

    try:
        data = parse_session_log(dest_path)
    except Exception as exc:
        return (
            jsonify({"ok": False, "message": f"Parse error: {exc}"}),
            400,
        )

    ok, msg = insert_session_data(data)
    status = 200 if ok else 400
    return jsonify({"ok": ok, "message": msg, "filename": filename}), status


@dashboard_bp.route("/api/quick_session", methods=["POST"])
def api_quick_session():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"ok": False, "message": "No JSON data provided."}), 400

        # Validate required fields
        required_fields = ["topic", "study_mode", "time_spent_minutes", "understanding_level"]
        for field in required_fields:
            if field not in data or data[field] in [None, ""]:
                return jsonify({"ok": False, "message": f"Missing required field: {field}"}), 400

        main_topic = (data.get("topic") or data.get("main_topic") or "").strip()
        duration_val = data.get("time_spent_minutes") or data.get("duration_minutes")
        if not main_topic:
            return jsonify({"ok": False, "message": "Missing required field: topic"}), 400
        if duration_val in [None, ""]:
            return jsonify({"ok": False, "message": "Missing required field: time_spent_minutes"}), 400

        # Map to v9.1 schema fields
        session_data = {
            "session_date": data.get("session_date", datetime.now().strftime("%Y-%m-%d")),
            "session_time": data.get("session_time", datetime.now().strftime("%H:%M")),
            "duration_minutes": int(duration_val),
            "study_mode": data["study_mode"],
            "target_exam": data.get("target_exam", "").strip(),
            "source_lock": data.get("source_lock", "").strip(),
            "plan_of_attack": data.get("plan_of_attack", "").strip(),
            "main_topic": main_topic,
            "subtopics": data.get("subtopics", "").strip(),
            "frameworks_used": data.get("frameworks_used", "").strip(),
            "gated_platter_triggered": data.get("gated_platter_triggered", "No"),
            "wrap_phase_reached": data.get("wrap_phase_reached", "No"),
            "anki_cards_count": int(data.get("anki_cards_count", 0)),
            "region_covered": data.get("region_covered", "").strip(),
            "landmarks_mastered": data.get("landmarks_mastered", "").strip(),
            "muscles_attached": data.get("muscles_attached", "").strip(),
            "oian_completed_for": data.get("oian_completed_for", "").strip(),
            "rollback_events": data.get("rollback_events", "").strip(),
            "drawing_used": data.get("drawing_used", "").strip(),
            "drawings_completed": data.get("drawings_completed", "").strip(),
            "understanding_level": int(data["understanding_level"]),
            "retention_confidence": int(data.get("retention_confidence", 3)),
            "system_performance": int(data.get("system_performance", 3)),
            "calibration_check": data.get("calibration_check", "").strip(),
            "anchors_locked": data.get("anchors_locked", "").strip(),
            "what_worked": data.get("what_worked", "").strip(),
            "what_needs_fixing": data.get("what_needs_fixing", "").strip(),
            "gaps_identified": data.get("gaps_identified", "").strip(),
            "notes_insights": data.get("notes_insights", "").strip(),
            "next_topic": data.get("next_topic", "").strip(),
            "next_focus": data.get("next_focus", "").strip(),
            "next_materials": data.get("next_materials", "").strip(),
            "created_at": datetime.now().isoformat(),
            "schema_version": "9.1",
        }

        # Validate score ranges
        for score_field in ["understanding_level", "retention_confidence", "system_performance"]:
            score = session_data[score_field]
            if score < SCORE_MIN or score > SCORE_MAX:
                return jsonify({"ok": False, "message": f"{score_field} must be between {SCORE_MIN} and {SCORE_MAX}"}), 400

        # Insert into database
        ok, msg = insert_session_data(session_data)
        status = 200 if ok else 400
        return jsonify({"ok": ok, "message": msg}), status

    except ValueError as e:
        return jsonify({"ok": False, "message": f"Invalid number format: {e}"}), 400
    except Exception as e:
        return jsonify({"ok": False, "message": f"Server error: {e}"}), 500


@dashboard_bp.route("/api/tutor/session/start", methods=["POST"])
def api_tutor_session_start():
    """Start a new Tutor session."""
    payload = request.get_json() or {}
    session_id = payload.get("session_id") or datetime.now().strftime(
        "sess-%Y%m%d-%H%M%S"
    )
    mode = payload.get("mode", "Core")
    course_id = payload.get("course_id")
    
    return jsonify({
        "ok": True, 
        "session_id": session_id,
        "mode": mode,
        "course_id": course_id,
        "message": f"Tutor session started in {mode} mode"
    })


@dashboard_bp.route("/api/tutor/rag-docs", methods=["GET"])
def api_tutor_list_rag_docs():
    """List RAG documents for Source-Lock selection in the Tutor UI."""
    init_database()
    limit = int(request.args.get("limit") or 200)
    limit = max(1, min(limit, 1000))
    search = (request.args.get("search") or "").strip()
    doc_type = (request.args.get("doc_type") or "").strip()
    corpus = (request.args.get("corpus") or "").strip().lower()

    conditions: list[str] = []
    params: list[object] = []

    if doc_type:
        conditions.append("doc_type = ?")
        params.append(doc_type)

    if corpus:
        conditions.append("COALESCE(corpus, 'runtime') = ?")
        params.append(corpus)

    if search:
        like = f"%{search}%"
        conditions.append("(source_path LIKE ? OR topic_tags LIKE ? OR content LIKE ?)")
        params.extend([like, like, like])

    where_clause = " AND ".join(conditions) if conditions else "1=1"

    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute(
        f"""
        SELECT id, source_path, doc_type, topic_tags, course_id, created_at, updated_at,
               COALESCE(corpus, 'runtime') as corpus, COALESCE(folder_path, '') as folder_path,
               COALESCE(enabled, 1) as enabled
        FROM rag_docs
        WHERE {where_clause}
        ORDER BY id DESC
        LIMIT ?
        """,
        (*params, limit),
    )
    rows = cur.fetchall()
    conn.close()

    docs = [
        {
            "id": int(r["id"]),
            "source_path": r["source_path"],
            "doc_type": r["doc_type"],
            "topic_tags": r["topic_tags"] or "",
            "course_id": r["course_id"],
            "created_at": r["created_at"],
            "updated_at": r["updated_at"],
            "corpus": r["corpus"],
            "folder_path": r["folder_path"],
            "enabled": int(r["enabled"]),
        }
        for r in rows
    ]

    return jsonify({"ok": True, "docs": docs})


@dashboard_bp.route("/api/tutor/project-files/upload", methods=["POST"])
def api_tutor_upload_project_file():
    """Deprecated: file uploads are disabled (Study RAG is folder-synced)."""
    return jsonify({
        "ok": False,
        "message": "File uploads are disabled. Drop files into brain/data/study_rag and click Sync. Use 'Add Link' for YouTube/URLs.",
    }), 410


@dashboard_bp.route("/api/tutor/study/sync", methods=["POST"])
def api_tutor_sync_study_folder():
    """Sync the Study RAG drop-folder (brain/data/study_rag) into rag_docs."""
    try:
        result = sync_folder_to_rag(str(STUDY_RAG_PATH), corpus="study")
        return jsonify({
            "ok": True,
            "root": result.get("root"),
            "processed": result.get("processed", 0),
            "errors": result.get("errors", []),
            "message": f"Synced Study folder. Files processed: {result.get('processed', 0)}",
        })
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Study sync failed: {exc}"}), 500


@dashboard_bp.route("/api/tutor/study/folders", methods=["GET"])
def api_tutor_list_study_folders():
    """List Study RAG folders (relative) with enabled status and doc counts."""
    init_database()
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute(
        """
        SELECT COALESCE(folder_path, '') as folder_path,
               COUNT(*) as doc_count,
               MIN(COALESCE(enabled, 1)) as enabled
        FROM rag_docs
        WHERE COALESCE(corpus, 'runtime') = 'study'
        GROUP BY COALESCE(folder_path, '')
        ORDER BY folder_path
        """
    )
    rows = cur.fetchall()
    conn.close()
    folders = [
        {
            "folder_path": r["folder_path"],
            "doc_count": int(r["doc_count"]),
            "enabled": int(r["enabled"]),
        }
        for r in rows
    ]
    return jsonify({"ok": True, "root": str(STUDY_RAG_PATH), "folders": folders})


@dashboard_bp.route("/api/tutor/study/folders/set", methods=["POST"])
def api_tutor_set_study_folder_enabled():
    """Enable/disable a Study folder (applies recursively)."""
    payload = request.get_json() or {}
    folder_path = (payload.get("folder_path") or "").strip().replace("\\", "/")
    enabled = 1 if payload.get("enabled") in (True, 1, "1", "true", "True") else 0

    init_database()
    conn = sqlite3.connect(str(DB_PATH))
    cur = conn.cursor()

    if folder_path:
        like_prefix = folder_path.rstrip("/") + "/%"
        cur.execute(
            """
            UPDATE rag_docs
            SET enabled = ?, updated_at = ?
            WHERE COALESCE(corpus, 'runtime') = 'study'
              AND (COALESCE(folder_path, '') = ? OR COALESCE(folder_path, '') LIKE ?)
            """,
            (enabled, datetime.now().isoformat(timespec="seconds"), folder_path, like_prefix),
        )
    else:
        cur.execute(
            """
            UPDATE rag_docs
            SET enabled = ?, updated_at = ?
            WHERE COALESCE(corpus, 'runtime') = 'study'
            """,
            (enabled, datetime.now().isoformat(timespec="seconds")),
        )

    conn.commit()
    conn.close()
    return jsonify({"ok": True, "message": f"Study folder '{folder_path or '[root]'}' set to enabled={enabled}"})


@dashboard_bp.route("/api/tutor/runtime/items", methods=["GET"])
def api_tutor_list_runtime_items():
    """List Runtime catalog items (systems/engines) with enable toggles."""
    init_database()

    # Ensure runtime catalog exists at least once
    repo_root = Path(__file__).parent.parent.parent.resolve()
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute(
        """
        SELECT COUNT(*) as n
        FROM rag_docs
        WHERE COALESCE(corpus, 'runtime') = 'runtime'
          AND source_path LIKE 'runtime://%'
        """
    )
    n = int(cur.fetchone()["n"])
    conn.close()
    if n == 0:
        try:
            sync_runtime_catalog(str(repo_root))
        except Exception:
            # If it fails, still return an empty list.
            pass

    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    cur.execute(
        """
        SELECT id, source_path, folder_path, content, COALESCE(enabled, 1) as enabled
        FROM rag_docs
        WHERE COALESCE(corpus, 'runtime') = 'runtime'
          AND source_path LIKE 'runtime://%'
        ORDER BY COALESCE(folder_path, ''), source_path
        """
    )
    rows = cur.fetchall()
    conn.close()

    items = []
    for r in rows:
        source_path = r["source_path"] or ""
        key = source_path.replace("runtime://", "", 1)
        content = (r["content"] or "").strip()
        # description = second paragraph-ish; for UI keep short
        desc = ""
        parts = [p.strip() for p in content.split("\n\n") if p.strip()]
        if len(parts) >= 2:
            desc = parts[1]
        elif parts:
            desc = parts[0]
        if len(desc) > 240:
            desc = desc[:237].rstrip() + "..."
        items.append({
            "id": int(r["id"]),
            "key": key,
            "group": (r["folder_path"] or "Runtime"),
            "enabled": int(r["enabled"]),
            "description": desc,
        })

    return jsonify({"ok": True, "items": items})


@dashboard_bp.route("/api/tutor/runtime/items/set", methods=["POST"])
def api_tutor_set_runtime_item_enabled():
    payload = request.get_json() or {}
    try:
        doc_id = int(payload.get("id"))
    except Exception:
        return jsonify({"ok": False, "message": "id must be an integer"}), 400
    enabled = 1 if payload.get("enabled") in (True, 1, "1", "true", "True") else 0

    init_database()
    conn = sqlite3.connect(str(DB_PATH))
    cur = conn.cursor()
    cur.execute(
        """
        UPDATE rag_docs
        SET enabled = ?, updated_at = ?
        WHERE id = ?
          AND COALESCE(corpus, 'runtime') = 'runtime'
        """,
        (enabled, datetime.now().isoformat(timespec="seconds"), doc_id),
    )
    conn.commit()
    conn.close()
    return jsonify({"ok": True, "message": f"Runtime item {doc_id} set to enabled={enabled}"})


@dashboard_bp.route("/api/tutor/links/add", methods=["POST"])
def api_tutor_add_link_doc():
    """Add a URL (e.g., YouTube link) into rag_docs for Source-Lock selection."""
    payload = request.get_json() or {}
    url = (payload.get("url") or "").strip()
    if not url:
        return jsonify({"ok": False, "message": "url is required"}), 400
    if not (url.startswith("http://") or url.startswith("https://")):
        return jsonify({"ok": False, "message": "url must start with http:// or https://"}), 400

    doc_type = (payload.get("doc_type") or "youtube").strip().lower() or "youtube"
    allowed_doc_types = {
        "note",
        "textbook",
        "transcript",
        "slide",
        "other",
        "powerpoint",
        "pdf",
        "txt",
        "mp4",
        "youtube",
    }
    if doc_type not in allowed_doc_types:
        doc_type = "youtube"

    raw_tags = (payload.get("tags") or "").strip()
    tags: list[str] = ["link"]
    if raw_tags:
        tags.extend([t.strip() for t in re.split(r"[,\n]", raw_tags) if t.strip()])

    course_id_raw = (payload.get("course_id") or "").strip()
    course_id = None
    if course_id_raw:
        try:
            course_id = int(course_id_raw)
        except ValueError:
            return jsonify({"ok": False, "message": "course_id must be an integer"}), 400

    try:
        doc_id = ingest_url_document(
            url=url,
            doc_type=doc_type,
            course_id=course_id,
            topic_tags=tags,
            content=(payload.get("content") or "").strip(),
            corpus="study",
            folder_path="links",
        )
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Ingest failed: {exc}"}), 500

    return jsonify({
        "ok": True,
        "doc_id": doc_id,
        "doc_type": doc_type,
        "tags": tags,
        "message": f"Added link and ingested as RAG doc {doc_id}",
    })


@dashboard_bp.route("/api/tutor/session/turn", methods=["POST"])
def api_tutor_session_turn():
    """Process a Tutor conversation turn using Codex + RAG."""
    payload = request.get_json() or {}
    try:
        # Build source selector from payload
        sources = TutorSourceSelector(
            allowed_doc_ids=payload.get("sources", {}).get("allowed_doc_ids", []),
            allowed_kinds=payload.get("sources", {}).get("allowed_kinds", []),
            disallowed_doc_ids=payload.get("sources", {}).get(
                "disallowed_doc_ids", []
            ),
        )
        
        # Build query object
        query = TutorQueryV1(
            user_id=payload.get("user_id", "default"),
            session_id=payload.get("session_id"),
            course_id=payload.get("course_id"),
            topic_id=payload.get("topic_id"),
            mode=payload.get("mode", "Core"),
            question=payload.get("question", ""),
            plan_snapshot_json=payload.get("plan_snapshot_json", "{}"),
            sources=sources,
            notes_context_ids=payload.get("notes_context_ids", []),
        )
        
        if not query.question.strip():
            return jsonify({"ok": False, "message": "Question cannot be empty"}), 400
        
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Invalid TutorQuery payload: {exc}"}), 400

    # Process the turn through the Tutor engine
    try:
        response = process_tutor_turn(query)
        
        # Log the turn to database
        turn_id = log_tutor_turn(query, response)
        
        # Convert citations to dict format
        citations_list = [
            {
                "doc_id": c.doc_id,
                "source_path": c.source_path,
                "doc_type": c.doc_type,
                "snippet": c.snippet
            }
            for c in response.citations
        ]
        
        return jsonify({
            "ok": True,
            "session_id": response.session_id,
            "turn_id": turn_id,
            "answer": response.answer,
            "citations": citations_list,
            "unverified": response.unverified,
            "summary": json.loads(response.summary_json) if response.summary_json else None
        })
        
    except Exception as exc:
        return jsonify({
            "ok": False, 
            "message": f"Tutor error: {exc}",
            "session_id": query.session_id or "error"
        }), 500


@dashboard_bp.route("/api/tutor/card-draft", methods=["POST"])
def api_tutor_create_card_draft():
    """Create a card draft from a Tutor turn."""
    payload = request.get_json() or {}
    
    turn_id = payload.get("turn_id")
    front = payload.get("front", "").strip()
    back = payload.get("back", "").strip()
    hook = payload.get("hook", "").strip() or None
    tags = payload.get("tags", "").strip() or None
    
    if not turn_id:
        return jsonify({"ok": False, "message": "turn_id is required"}), 400
    if not front or not back:
        return jsonify({"ok": False, "message": "front and back are required"}), 400
    
    try:
        card_id = create_card_draft_from_turn(turn_id, front, back, hook, tags)
        return jsonify({
            "ok": True,
            "card_id": card_id,
            "message": "Card draft created"
        })
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Error creating card: {exc}"}), 500


@dashboard_bp.route("/api/syllabus/import", methods=["POST"])
def api_syllabus_import():
    payload = request.get_json() or {}
    try:
        course_data = {
            "name": (payload.get("name") or "").strip(),
            "code": (payload.get("code") or "").strip(),
            "term": (payload.get("term") or "").strip(),
            "instructor": (payload.get("instructor") or "").strip(),
            "default_study_mode": (payload.get("default_study_mode") or "Core").strip()
            or "Core",
            "time_budget_per_week_minutes": int(
                payload.get("time_budget_per_week_minutes") or 0
            ),
        }
        event = {
            "type": (payload.get("event_type") or "other").strip(),
            "title": (payload.get("event_title") or "").strip(),
            "date": (payload.get("event_date") or "").strip() or None,
            "due_date": (payload.get("event_due_date") or "").strip() or None,
            "weight": float(payload.get("event_weight") or 0.0),
            "raw_text": (payload.get("event_raw_text") or "").strip(),
        }
        if not event["title"]:
            return (
                jsonify({"ok": False, "message": "Event title is required."}),
                400,
            )

        course_id = upsert_course(course_data)
        inserted = import_events(course_id, [event], replace=False)
        return jsonify(
            {
                "ok": True,
                "course_id": course_id,
                "events_added": inserted,
                "message": f"Course saved and {inserted} event(s) added.",
            }
        )
    except Exception as exc:
        return (
            jsonify({"ok": False, "message": f"Syllabus import failed: {exc}"}),
            400,
        )


@dashboard_bp.route("/api/syllabus/import_bulk", methods=["POST"])
def api_syllabus_import_bulk():
    payload = request.get_json() or {}
    print(f"[DEBUG] api_syllabus_import_bulk payload type: {type(payload)}")
    
    try:
        # Handle list of courses (bulk)
        if isinstance(payload, list):
             print(f"[DEBUG] Received list of {len(payload)} courses")
             total_events = 0
             course_ids = []
             for course_data in payload:
                 if not isinstance(course_data, dict): continue
                 try:
                     cid = upsert_course(course_data)
                     events = course_data.get("events") or []
                     inserted = import_events(cid, events, replace=False)
                     total_events += inserted
                     course_ids.append(cid)
                 except Exception as e:
                     print(f"[ERROR] Failed to import course: {e}")
             
             return jsonify({
                 "ok": True,
                 "courses_processed": len(course_ids),
                 "events_added": total_events,
                 "message": f"Bulk import processed {len(course_ids)} courses and {total_events} events."
             })
             
        # Handle single course (original behavior but safer)
        if "name" in payload:
            print(f"[DEBUG] Received single course: {payload.get('name')}")
            course_id = upsert_course(payload)
            events = payload.get("events") or []
            inserted = import_events(course_id, events, replace=False)
            return jsonify(
                {
                    "ok": True,
                    "course_id": course_id,
                    "events_added": inserted,
                    "message": f"Bulk syllabus import complete: {inserted} event(s) added.",
                }
            )
        else:
             print("[DEBUG] Payload missing 'name', and is not a list.")
             return jsonify({"ok": False, "message": "Invalid JSON: Expected course object or list of courses."}), 400
             
    except Exception as exc:
        print(f"[ERROR] api_syllabus_import_bulk exception: {exc}")
        import traceback
        traceback.print_exc()
        return (
            jsonify({"ok": False, "message": f"Bulk syllabus import failed: {exc}"}),
            400,
        )


@dashboard_bp.route("/api/syllabus/courses")
def api_syllabus_courses():
    courses, events = fetch_all_courses_and_events()
    if not courses:
        return jsonify({"courses": [], "summary": {}})

    # Basic per-course counts
    events_by_course = {}
    for ev in events:
        cid = ev["course_id"]
        events_by_course.setdefault(cid, []).append(ev)

    today = datetime.now().date()
    summary_courses = []
    for c in courses:
        course_events = events_by_course.get(c["id"], [])
        total_events = len(course_events)
        upcoming_7 = 0
        upcoming_30 = 0
        exams_count = 0

        for ev in course_events:
            ev_type = (ev.get("type") or "").lower()
            date_str = ev.get("date") or ev.get("due_date")
            ev_date = None
            if date_str:
                try:
                    ev_date = datetime.strptime(date_str, "%Y-%m-%d").date()
                except Exception:
                    ev_date = None

            if ev_type == "exam":
                exams_count += 1

            if ev_date and ev_date >= today:
                delta = (ev_date - today).days
                if delta <= 7:
                    upcoming_7 += 1
                if delta <= 30:
                    upcoming_30 += 1

        summary_courses.append(
            {
                **c,
                "events_total": total_events,
                "upcoming_7_days": upcoming_7,
                "upcoming_30_days": upcoming_30,
                "exams_count": exams_count,
            }
        )

    overall_summary = {
        "courses_count": len(courses),
        "events_count": len(events),
    }

    return jsonify(
        {
            "courses": summary_courses,
            "summary": overall_summary,
        }
    )


@dashboard_bp.route("/api/syllabus/events")
def api_syllabus_events():
    course_id_raw = request.args.get("course_id")
    try:
        course_id = int(course_id_raw) if course_id_raw else None
    except ValueError:
        return jsonify({"ok": False, "message": "Invalid course_id"}), 400

    courses, events = fetch_all_courses_and_events()
    if course_id is not None:
        events = [ev for ev in events if ev["course_id"] == course_id]

    raw_sessions = get_all_sessions()
    sessions = [dict(s) for s in raw_sessions]

    enriched_events = attach_event_analytics(events, sessions)

    return jsonify(
        {
            "ok": True,
            "events": enriched_events,
            "thresholds": {
                "weak": WEAK_THRESHOLD,
                "strong": STRONG_THRESHOLD,
                "stale_days": STALE_DAYS,
                "fresh_days": FRESH_DAYS,
            },
        }
    )


@dashboard_bp.route("/api/calendar/data")
def api_calendar_data():
    start_date_str = request.args.get("start_date")
    end_date_str = request.args.get("end_date")
    course_id_raw = request.args.get("course_id")
    event_type = request.args.get("event_type")

    today = datetime.now().date()
    start_date = (
        datetime.strptime(start_date_str, "%Y-%m-%d").date()
        if start_date_str
        else today - timedelta(days=7)
    )
    end_date = (
        datetime.strptime(end_date_str, "%Y-%m-%d").date()
        if end_date_str
        else today + timedelta(days=60)
    )

    try:
        course_id = int(course_id_raw) if course_id_raw else None
    except ValueError:
        course_id = None

    result = get_calendar_data(start_date, end_date, course_id, event_type)
    return jsonify(result)


@dashboard_bp.route("/api/calendar/plan_session", methods=["POST"])
def api_calendar_plan_session():
    payload = request.get_json() or {}
    try:
        course_id = payload.get("course_id")
        topic_id = payload.get("topic_id")
        course_event_id = payload.get("course_event_id")
        scheduled_date = payload.get("scheduled_date", "").strip()
        planned_minutes = int(payload.get("planned_minutes") or 60)
        notes = (payload.get("notes") or "").strip()

        if not scheduled_date:
            return jsonify({"ok": False, "message": "scheduled_date is required"}), 400

        try:
            datetime.strptime(scheduled_date, "%Y-%m-%d")
        except ValueError:
            return jsonify({"ok": False, "message": "Invalid date format (use YYYY-MM-DD)"}), 400

        init_database()
        conn = get_connection()
        cur = conn.cursor()

        now = datetime.now().isoformat(timespec="seconds")
        cur.execute(
            """
            INSERT INTO study_tasks (
                course_id, topic_id, course_event_id,
                scheduled_date, planned_minutes, status, notes,
                created_at
            )
            VALUES (?, ?, ?, ?, ?, 'pending', ?, ?)
            """,
            (course_id, topic_id, course_event_id, scheduled_date, planned_minutes, notes, now),
        )
        task_id = cur.lastrowid
        conn.commit()
        conn.close()

        return jsonify(
            {
                "ok": True,
                "task_id": task_id,
                "message": f"Planned session created for {scheduled_date}",
            }
        )
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Failed to create planned session: {exc}"}), 400


# ---------------------------------------------------------------------------
# Card Draft Endpoints (Tutor -> Anki pipeline)
# ---------------------------------------------------------------------------

@dashboard_bp.route("/api/cards/draft", methods=["POST"])
def api_create_card_draft():
    """
    Create a new card draft from Tutor.
    
    Expected JSON payload:
        session_id: str (e.g., "sess-20260109-143022")
        course_id: int (optional)
        topic_id: int (optional)
        deck_name: str (default "PT Study::Default")
        card_type: str ("basic", "cloze", "image_occlusion")
        front: str (required)
        back: str (required)
        tags: str (comma-separated, optional)
        source_citation: str (optional)
    
    Returns:
        JSON with ok, card_id, message
    """
    payload = request.get_json() or {}
    
    try:
        # Required fields
        front = (payload.get("front") or "").strip()
        back = (payload.get("back") or "").strip()
        
        if not front or not back:
            return jsonify({"ok": False, "message": "front and back are required"}), 400
        
        # Optional fields with defaults
        session_id = payload.get("session_id")
        course_id = payload.get("course_id")
        topic_id = payload.get("topic_id")
        deck_name = payload.get("deck_name", "PT Study::Default")
        card_type = payload.get("card_type", "basic")
        tags = payload.get("tags", "")
        source_citation = payload.get("source_citation", "")
        
        # Validate card_type
        valid_types = ("basic", "cloze", "image_occlusion")
        if card_type not in valid_types:
            return jsonify({
                "ok": False, 
                "message": f"card_type must be one of: {', '.join(valid_types)}"
            }), 400
        
        init_database()
        conn = get_connection()
        cur = conn.cursor()
        
        now = datetime.now().isoformat(timespec="seconds")
        cur.execute(
            """
            INSERT INTO card_drafts (
                session_id, course_id, topic_id, deck_name,
                card_type, front, back, tags, source_citation,
                status, created_at
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 'pending', ?)
            """,
            (session_id, course_id, topic_id, deck_name,
             card_type, front, back, tags, source_citation, now)
        )
        card_id = cur.lastrowid
        conn.commit()
        conn.close()
        
        return jsonify({
            "ok": True,
            "card_id": card_id,
            "message": "Card draft created"
        })
        
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Failed to create card draft: {exc}"}), 400


@dashboard_bp.route("/api/cards/drafts", methods=["GET"])
def api_list_card_drafts():
    """
    List card drafts with optional filtering.
    
    Query params:
        status: Filter by status (pending, approved, rejected, synced)
        course_id: Filter by course
        limit: Max results (default 100)
    
    Returns:
        JSON with ok, drafts (list), count
    """
    try:
        status_filter = request.args.get("status")
        course_id = request.args.get("course_id")
        limit = int(request.args.get("limit", 100))
        
        init_database()
        conn = get_connection()
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()
        
        query = "SELECT * FROM card_drafts WHERE 1=1"
        params = []
        
        if status_filter:
            query += " AND status = ?"
            params.append(status_filter)
        
        if course_id:
            query += " AND course_id = ?"
            params.append(int(course_id))
        
        query += " ORDER BY created_at DESC LIMIT ?"
        params.append(limit)
        
        cur.execute(query, params)
        rows = cur.fetchall()
        conn.close()
        
        drafts = [dict(row) for row in rows]
        
        return jsonify({
            "ok": True,
            "drafts": drafts,
            "count": len(drafts)
        })
        
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Failed to list drafts: {exc}"}), 400


@dashboard_bp.route("/api/cards/drafts/<int:draft_id>", methods=["PATCH"])
def api_update_card_draft(draft_id: int):
    """
    Update a card draft (approve, reject, edit).
    
    Expected JSON payload:
        status: "approved" | "rejected" | "pending"
        front: str (optional - update content)
        back: str (optional - update content)
        tags: str (optional)
    
    Returns:
        JSON with ok, message
    """
    payload = request.get_json() or {}
    
    try:
        init_database()
        conn = get_connection()
        cur = conn.cursor()
        
        # Check draft exists
        cur.execute("SELECT id, status FROM card_drafts WHERE id = ?", (draft_id,))
        row = cur.fetchone()
        
        if not row:
            conn.close()
            return jsonify({"ok": False, "message": "Draft not found"}), 404
        
        current_status = row[1]
        
        # Build update query dynamically
        updates = []
        params = []
        
        if "status" in payload:
            new_status = payload["status"]
            valid_statuses = ("pending", "approved", "rejected", "synced")
            if new_status not in valid_statuses:
                conn.close()
                return jsonify({
                    "ok": False,
                    "message": f"status must be one of: {', '.join(valid_statuses)}"
                }), 400
            updates.append("status = ?")
            params.append(new_status)
        
        if "front" in payload:
            updates.append("front = ?")
            params.append(payload["front"])
        
        if "back" in payload:
            updates.append("back = ?")
            params.append(payload["back"])
        
        if "tags" in payload:
            updates.append("tags = ?")
            params.append(payload["tags"])
        
        if not updates:
            conn.close()
            return jsonify({"ok": False, "message": "No fields to update"}), 400
        
        query = f"UPDATE card_drafts SET {', '.join(updates)} WHERE id = ?"
        params.append(draft_id)
        
        cur.execute(query, params)
        conn.commit()
        conn.close()
        
        return jsonify({
            "ok": True,
            "message": f"Draft {draft_id} updated"
        })
        
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Failed to update draft: {exc}"}), 400


@dashboard_bp.route("/api/cards/drafts/pending", methods=["GET"])
def api_pending_card_drafts():
    """
    Get count and list of pending card drafts ready for review.
    
    Returns:
        JSON with ok, pending_count, drafts (list)
    """
    try:
        init_database()
        conn = get_connection()
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()
        
        cur.execute(
            """
            SELECT * FROM card_drafts 
            WHERE status = 'pending' 
            ORDER BY created_at DESC
            LIMIT 50
            """
        )
        rows = cur.fetchall()
        
        cur.execute("SELECT COUNT(*) FROM card_drafts WHERE status = 'pending'")
        total_pending = cur.fetchone()[0]
        
        conn.close()
        
        drafts = [dict(row) for row in rows]
        
        return jsonify({
            "ok": True,
            "pending_count": total_pending,
            "drafts": drafts
        })
        
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Failed to get pending drafts: {exc}"}), 400


@dashboard_bp.route("/api/cards/sync", methods=["POST"])
def api_sync_cards_to_anki():
    """
    Sync approved card drafts to Anki via Anki Connect.
    
    Expected JSON payload (optional):
        dry_run: bool (default False) - Preview without syncing
    
    Returns:
        JSON with ok, synced_count, errors, message
    """
    payload = request.get_json() or {}
    dry_run = payload.get("dry_run", False)
    
    try:
        # Import anki_sync module
        import sys
        from pathlib import Path
        brain_path = Path(__file__).parent.parent
        if str(brain_path) not in sys.path:
            sys.path.insert(0, str(brain_path))
        
        from anki_sync import sync_pending_cards
        
        result = sync_pending_cards(dry_run=dry_run)
        
        return jsonify({
            "ok": True,
            "synced_count": result.get("synced", 0),
            "errors": result.get("errors", []),
            "message": f"Synced {result.get('synced', 0)} cards to Anki"
        })
        
    except ImportError as exc:
        return jsonify({
            "ok": False,
            "message": f"Anki sync module not available: {exc}"
        }), 500
    except Exception as exc:
        return jsonify({"ok": False, "message": f"Sync failed: {exc}"}), 400
