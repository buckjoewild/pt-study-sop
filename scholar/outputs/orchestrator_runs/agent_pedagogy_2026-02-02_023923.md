## Summary
The telemetry snapshot is empty, so the highest‑leverage move is to pinpoint which pedagogy claims are currently unverified and what evidence must be logged to audit them against the rubric.

## Questions Needed
- With no telemetry snapshot available (signal), what concrete log evidence proves “retrieval before explanation” is enforced for every content chunk (rubric: Retrieval Practice checklist)?
- Where is the “next review” timestamp written, and what evidence shows Wrap phase scheduling happens for each session (rubric: Spaced Practice; rule: “Wrap phase explicitly schedule a follow‑up”)?
- What data shows topic alternation within a session to demonstrate interleaving rather than blocking (rubric: Interleaving / Discrimination; signal: no telemetry)?
- Do logs capture feedback timing and corrective content for incorrect responses, and what is the acceptable latency threshold (rubric: Feedback Quality; signal: missing telemetry)?
- How are errors labeled (e.g., Recall vs Conceptual) in logs, and what evidence shows rollback rules for repeated errors (rubric: Errorful Learning & Error Typing)?
- What proportion of prompts are transfer‑level (vignettes/“what happens if injured”) versus pure recall, and where is that recorded (rubric: Transfer & Application)?
- What operational metric shows cognitive load is controlled (e.g., instruction length, step count, cue fading), and how is it captured (rubric: Cognitive Load Management)?
- Is confidence captured per response, and do logs allow calibration checks (accuracy vs confidence) (rubric: Metacognition & Calibration)?
- How is Source‑Lock M0 enforced when a Source Packet is missing, and where is refusal/deferral behavior logged (rubric: Source Grounding)?
- What evidence shows efficiency gains (skipping mastered steps) and explicit session progress visibility (rubric: Efficiency)?

## Suggested Measurements
- `% of content chunks where question timestamp precedes explanation timestamp`
- `count of scheduled follow‑ups per session` and `median days‑to‑next‑review`
- `topic switch rate per session` and `interleaving entropy score`
- `median feedback latency after incorrect response` and `% corrective feedback vs binary judgment`
- `error type distribution (Recall/Conceptual/etc.)` and `repeat‑error rollback rate`
- `% of prompts tagged as transfer/vignette` vs recall
- `median instruction token count per prompt` and `step count per interaction` (load proxy)
- `confidence‑accuracy calibration (e.g., Brier score) per session`
- `Source‑Lock compliance rate` and `count of uncited factual claims`
- `% of steps skipped for mastered items` and `session progress visibility rate`